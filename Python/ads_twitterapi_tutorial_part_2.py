# -*- coding: utf-8 -*-
"""ADS_TwitterAPI_Tutorial (Part 2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/132jxFz1gy_w_SUMD5grV3jqHY0wVUQFw

# IU Applied Data Science (INFO_I-590)
### Instructor: Joanne Luciano ; TAs: Jeremy Yang and Kaicheng Yang
# ADS Twitter API Tutorial (Part 2): Feature vectorization and other text processing details
### Based on original tutorial and code by former TA JT Wolohan.
### *NOTE: This tutorial code does NOT fulfill all requrements of assignment.  It provides a good start!*
### References:

*   https://apps.twitter.com/
*   https://github.com/geduldig/TwitterAPI
*   https://media.readthedocs.org/pdf/twitterapi/latest/twitterapi.pdf

### Import packages
"""

import os, json
import pandas
import numpy
import sklearn
import sklearn.feature_extraction
import sklearn.model_selection 
import sklearn.metrics 
import sklearn.naive_bayes
import sklearn.svm
import sklearn.neighbors
import sklearn.neural_network
!pip install TwitterAPI
from TwitterAPI import TwitterAPI, TwitterOAuth


# Credentials file format:
# consumer_key=YOUR_CONSUMER_KEY
# consumer_secret=YOUR_CONSUMER_SECRET
# access_token_key=YOUR_ACCESS_TOKEN
# access_token_secret=YOUR_ACCESS_TOKEN_SECRET

"""### Connect and authenticate with Twitter REST API."""

o = TwitterOAuth.read_file(os.environ['HOME']+'/.twitterapi_credentials')
twapi = TwitterAPI(o.consumer_key, o.consumer_secret, o.access_token_key, o.access_token_secret)

"""### Convenience function"""

def searchTwitter(q, api, feed="search/tweets", n=100):
  return [t for t in api.request(feed, {'q':q,'count':n})]

"""### Get JSON from Twitter"""

cat_tweets = searchTwitter('#cats', twapi, n=100)
dog_tweets = searchTwitter('#dogs', twapi, n=100)

"""### Convert the json returned by Twitter into a dataframe"""

cat_df = pandas.read_json(json.dumps(cat_tweets))
dog_df = pandas.read_json(json.dumps(dog_tweets))

"""### Inspect data frames."""

pandas.set_option('display.max_columns', None) 
cat_df.head()
dog_df.head()

cat_df.loc[1]

pandas.set_option('display.max_colwidth', 100)
cat_df.loc[1:10]['text']

"""### Get text only and replace hashtags with blanks
If you want to use the normalizer, import it above and pass x.replace() to the noramlizer function
"""

cat_txt = [x.replace('#cats',"BLAH") for x in cat_df['text']]
dog_txt = [x.replace('#dogs',"BLAH") for x in dog_df['text']]

cat_txt[1:10]

"""### Vectorization, Feature Selection and Extraction
* Train Scikit-Learn CountVectorizer on corpus of tweets, to define terms. 
* Extract term-counts and generate term-document matrices.
"""

vectorizer = sklearn.feature_extraction.text.CountVectorizer(cat_txt+dog_txt, analyzer='word', stop_words=None, min_df=5)
vectorizer.fit(cat_txt+dog_txt)
cat_tdm = vectorizer.transform(cat_txt).toarray()
dog_tdm = vectorizer.transform(dog_txt).toarray()

vectorizer.get_feature_names()

cat_tdm[1:5]

"""### Combine matricies, adding class labels for training and testing."""

catdog_tdm = numpy.concatenate((cat_tdm,dog_tdm),axis=0)
Ycat = numpy.array([0 for i in range(len(cat_txt))])
Ydog = numpy.array([1 for i in range(len(dog_txt))])
Y = numpy.concatenate((Ycat,Ydog),axis=0)

"""### Redo with stopwords list from NLTK
NLTK = Natural Language Toolkit (https://www.nltk.org/)
"""

import nltk
import nltk.corpus
nltk.download('stopwords')
stopwds = list(nltk.corpus.stopwords.words('english'))
stopwds

vectorizer = sklearn.feature_extraction.text.CountVectorizer(cat_txt+dog_txt, analyzer='word', stop_words=stopwds, min_df=5)
vectorizer.fit(cat_txt+dog_txt)
cat_tdm = vectorizer.transform(cat_txt).toarray()
dog_tdm = vectorizer.transform(dog_txt).toarray()
vectorizer.get_feature_names()
